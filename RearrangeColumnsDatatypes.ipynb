{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Redfining features' data-types.  \n",
    "  \n",
    "After some exploration on the dataset I noticed the following :  \n",
    "- All values are set to float even the categorical and boolean.\n",
    "- Some of the categorical features are boolean features ( two categories 0 and 1).\n",
    "- Some of the continues features could be categorized as they have a few number of distinct values.\n",
    "\n",
    "----\n",
    "\n",
    "So what I'm going to do is to define a new data-set with some changes :  \n",
    "- analyse each feature and categorize it either [ continues, boolean or categorical ].\n",
    "- change features names to the format ps_(_cat, bin or cont)_...the rest of the old name ..., the reason for that is to make it easier to find the feature's category using the numpy.str.startwith().\n",
    "- I'll consider any feature with less than 30  distinct value to be categorical otherwise it's continues.\n",
    "- Also I'll change the data-type of them to be [ float for continues, bool for boolean and integer for categorical]\n",
    "- change the values of categorical items to be in range [ 0, number_of_distinct_values_for_this_feature ].\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from xgboost import DMatrix\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('./data/data')\n",
    "print('Data:\\t', data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get lists of features names by their data-type. [ [cont]inues and [cat]egorical ]\n",
    "counter = 0;\n",
    "datatypes = {\n",
    "    'cont' : [],\n",
    "    'cat' : []\n",
    "}\n",
    "\n",
    "def setDatatypes(d) :\n",
    "    global counter\n",
    "    title = data.columns[counter]\n",
    "    counter +=1;\n",
    "    if title in ['target','id'] : return\n",
    "    distictValues = d.value_counts().shape[0]\n",
    "    \n",
    "    if distictValues <= 30 :\n",
    "        datatypes['cat'].append(title)\n",
    "    else :\n",
    "        datatypes['cont'].append(title)\n",
    "        \n",
    "_ = data.apply(setDatatypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datatypes['cont']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def change_title(old_title, dtype) :\n",
    "    '''\n",
    "    change feature title in newData to format ps_(cat|cont)_.....\n",
    "    '''\n",
    "    pattern = re.compile('^(ps_)([a-z]{3,4}_[0-9]{1,2})')\n",
    "    t = pattern.findall(old_title)[0]\n",
    "    if len(t) == 2 :\n",
    "        new_title = t[0] + dtype + '_' + t[1]\n",
    "        data.rename(columns={old_title:new_title}, inplace=True)\n",
    "\n",
    "#-----------------------------------------------------------------------------------\n",
    "for feature in datatypes['cat'] :\n",
    "    categories = data[feature].value_counts().index.values\n",
    "    categories.sort()\n",
    "    data[feature] = pd.Categorical(data[feature]).codes\n",
    "    change_title(feature, 'cat')\n",
    "#--------------------------------------------------------------------------------------\n",
    "for feature in datatypes['cont'] :\n",
    "    change_title(feature, 'cont')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_categos = 0\n",
    "print (\"___________________________\\n\")\n",
    "print (\"categorical columns with number of categories :\\n\")\n",
    "for i in [ [col, data[col].value_counts().shape[0]]\\\n",
    "          for col in data.columns if col.startswith('ps_cat') ] :\n",
    "    \n",
    "    print (i[0], \" :\\t\", i[1])\n",
    "    total_categos += i[1]\n",
    "\n",
    "print('Total number of categories = ', total_categos)\n",
    "print(\"___________________________\\n\")\n",
    "print (\"continues columns with number of distinct values :\\n\")\n",
    "for i in [ [col, data[col].value_counts().shape[0]]\\\n",
    "          for col in data.columns if col.startswith('ps_cont') ] :\n",
    "    \n",
    "    print (i[0], \": \\tvalue_counts:\", i[1], '\\tMin:',data[i[0]].min().round(2), '\\tMax', data[i[0]].max().round(2),\n",
    "          '\\tMean:', data[i[0]].mean().round(2), '\\tMedian:', data[i[0]].median().round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# save data to disk\n",
    "#data.to_csv('./data/ready_data.csv', index=False)\n",
    "data = pd.read_csv('./data/ready_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "cont = [ feature for feature in data.columns if feature.startswith('ps_cont') ]\n",
    "fig, axs = plt.subplots(nrows=5, ncols=1, figsize=(8,6))\n",
    "\n",
    "for feature, ax in zip(cont, range(5)) :\n",
    "    norm = MinMaxScaler()\n",
    "    enc_data[feature] = norm.fit_transform(data[feature].values.reshape((-1,1)))\n",
    "    sns.distplot(enc_data[feature], rug=True, ax=axs[ax])\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define OneHotEncoder classifier.\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import pickle\n",
    "#encoders = []\n",
    "enc_data = data[cont].copy()\n",
    "\n",
    "cat = [ feature for feature in data.columns if feature.startswith('ps_cat') ]\n",
    "for feature in cat :\n",
    "        enc = OneHotEncoder(sparse=False)\n",
    "        encoded = enc.fit_transform(data[feature].values.reshape((-1,1)))\n",
    "        #encoders.append(enc)\n",
    "        enc_data = pd.concat((enc_data, pd.DataFrame(encoded)), axis=1)\n",
    "    \n",
    "#with open('./data/OneHotEncoder.clf', 'wb') as f:\n",
    "#    pickle.dump(file=f, obj=encoders)\n",
    "enc_data.columns = range(enc_data.columns.shape[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_data.to_csv('./data/ready_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr_data = enc_data.values\n",
    "np.save('./data/ready_data.npy', arr_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrT = enc_data.corr()\n",
    "corrT[corrT == 1] = 0\n",
    "corrT = corrT[corrT.abs() >= 0.8].dropna(how='all').dropna(how='all',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(corrT.shape[0]):\n",
    "    for j in range(i, corrT.shape[0]):\n",
    "        if not np.isnan(corrT.iloc[i,j]):\n",
    "            print(corrT.columns[i], '\\t&\\t ', corrT.columns[j],'\\t=\\t', corrT.iloc[i,j])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Duplicates:\n",
    " 31 32\n",
    "41 \t&\t  42\n",
    "43 \t&\t  44 \t\n",
    "45 \t&\t  46 \t\n",
    "47 \t&\t  48 \t\n",
    "49 \t&\t  50 \t\n",
    "51 \t&\t  52 \t\n",
    "53 \t&\t  54 \t\n",
    "53 \t&\t  57 \n",
    "54 \t&\t  57\n",
    "55 \t&\t  56 \t\n",
    "57 \t&\t  58 \t\n",
    "76 \t&\t  77 \t\n",
    "78 \t&\t  79 \t\n",
    "80 \t&\t  81 \t\n",
    "125 \t&\t  126\n",
    "164 \t&\t  165\n",
    "172 \t&\t  173 \n",
    "379 \t&\t  380 \n",
    "381 \t&\t  382 \n",
    "383 \t&\t  384 \n",
    "385 \t&\t  386 \n",
    "387 \t&\t  388 \n",
    "389 \t&\t  390\n",
    "\n",
    "\n",
    "\n",
    "42,44,46,48,50,52,54,57,56,58,77,79,81,126,165,173,380,382,384,386,388,390"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop cols with more than 93% correlation\n",
    "cols_to_drop = [42,44,46,48,50,52,54,57,56,58,77,79,81,126,165,173,380,382,384,386,388,390]\n",
    "enc_data.drop(columns=cols_to_drop, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr_data = enc_data.values\n",
    "np.save('./data/ready_data.npy', arr_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.load('./data/ready_data.npy')\n",
    "target = pd.read_csv('./data/target')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:  (595211, 369) \tTarget:  (595211, 1)\n",
      "Test: (892817, 369)\n"
     ]
    }
   ],
   "source": [
    "train = data[:target.shape[0]]\n",
    "test = data[target.shape[0]:]\n",
    "\n",
    "print('Train: ', train.shape, '\\tTarget: ', target.shape)\n",
    "print('Test:', test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.64 % of the labels are positive and  96.36 % is negative\n",
      "Accordingly I am going to take N validation set with 3.64% postive labels\n",
      "My validation set of size =  10000 will have 364 positives and 9636 negatives\n",
      "x_train:\t (585211, 369) \ty_train:\t (585211, 1)\n",
      "x_valid:\t (10000, 369) \ty_valid:\t (10000, 1)\n"
     ]
    }
   ],
   "source": [
    "# check implancing\n",
    "\n",
    "pos = (target['0'].value_counts()[1]/target.shape[0])\n",
    "print(pos.round(4)*100,'% of the labels are positive and ',(1-pos.round(4))*100,'% is negative')\n",
    "print('Accordingly I am going to take N validation set with 3.64% postive labels')\n",
    "\n",
    "valid_size = int(1e4)\n",
    "v_pos = int(valid_size * pos)\n",
    "v_neg = valid_size - v_pos\n",
    "print('My validation set of size = ',valid_size, 'will have', v_pos, 'positives and',v_neg, 'negatives')\n",
    "\n",
    "pos_ind = target[target['0']==1].index.tolist()\n",
    "neg_ind = target[target['0']==0].index.tolist()\n",
    "\n",
    "v_pos_ind = np.random.choice(pos_ind, v_pos, replace=False)\n",
    "v_neg_ind = np.random.choice(neg_ind, v_neg, replace=False)\n",
    "valid_ind = v_pos_ind.tolist() + v_neg_ind.tolist()\n",
    "train_ind = list(set(target.index.tolist()) - set(valid_ind))\n",
    "np.random.shuffle(valid_ind)\n",
    "np.random.shuffle(train_ind)\n",
    "\n",
    "target = target.values\n",
    "x_train = train[train_ind]\n",
    "y_train = target[train_ind]\n",
    "x_valid = train[valid_ind]\n",
    "y_valid = target[valid_ind]\n",
    "\n",
    "print('x_train:\\t',x_train.shape,'\\ty_train:\\t',y_train.shape)\n",
    "print('x_valid:\\t',x_valid.shape,'\\ty_valid:\\t',y_valid.shape)\n",
    "\n",
    "np.save('./data/x_train.npy',x_train)\n",
    "np.save('./data/y_train.npy',y_train)\n",
    "np.save('./data/x_valid.npy',x_valid)\n",
    "np.save('./data/y_valid.npy',y_valid)\n",
    "#np.save('./data/test.npy', test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting to DMAtrices\n",
    "dtrain = DMatrix(x_train, y_train, nthread=-1)\n",
    "dvalid = DMatrix(x_valid, y_valid, nthread=-1)\n",
    "dtest  = DMatrix(test, nthread=-1)\n",
    "\n",
    "del test, x_train, y_train, x_valid, y_valid, train, data\n",
    "\n",
    "dtrain.save_binary('./data/train.buffer')\n",
    "dvalid.save_binary('./data/valid.buffer')\n",
    "dtest.save_binary('./data/test.buffer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dtrain.save_binary('./data/train.buffer')\n",
    "dvalid.save_binary('./data/valid.buffer')\n",
    "dtest.save_binary('./data/test.buffer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note saving as csv takes way less memory than Dmatrix or npy format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
