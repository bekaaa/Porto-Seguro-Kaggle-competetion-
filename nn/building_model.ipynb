{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import pandas as pd\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"last\"\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train = np.load('../nn/data/train_sample.npy')\n",
    "train = np.load('../nn/data/train_scaled.npy')\n",
    "#test_data  = np.load('../nn/data/test_scaled.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = train[:,0].reshape(-1,1)\n",
    "train = train[:,1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################################\n",
    "# convert data into train_true, train_false, and validation #\n",
    "#############################################################\n",
    "train_true  = train[labels[:,0] == 1]\n",
    "train_false = train[labels[:,0] == 0]\n",
    "\n",
    "valid_size_true = int(train_true.shape[0] * .1)\n",
    "valid_size_false = 10000 - valid_size_true\n",
    "\n",
    "idx_true = range(train_true.shape[0])\n",
    "np.random.shuffle(idx_true)\n",
    "\n",
    "valid_data = train_true[idx_true[0:valid_size_true], :]\n",
    "valid_labels = np.ones([valid_size_true,1])\n",
    "\n",
    "idx_false = range(train_false.shape[0])\n",
    "np.random.shuffle(idx_false)\n",
    "\n",
    "valid_data   = np.append(valid_data, train_false[idx_false[0:valid_size_false] , :], axis=0)\n",
    "valid_labels = np.append(valid_labels, np.zeros([valid_size_false,1]), axis=0 )\n",
    "\n",
    "train_true = train_true[valid_size_true:, :]\n",
    "train_false = train_false[valid_size_false:, :]\n",
    "train_labels_T = np.ones([train_true.shape[0], 1])\n",
    "train_labels_F = np.zeros([train_false.shape[0], 1])\n",
    "\n",
    "print \"all data :\\t\",train.shape\n",
    "print \"all labels:\\t\",labels.shape\n",
    "print \"train true data :\\t\",train_true.shape\n",
    "print \"train true labels:\\t\",train_labels_T.shape\n",
    "print \"train false data :\\t\",train_false.shape\n",
    "print \"train false labels:\\t\",train_labels_F.shape\n",
    "print \"valid data :\\t\",valid_data.shape\n",
    "print \"valid labels:\\t\",valid_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################################\n",
    "# convert data into train and validation #\n",
    "##########################################\n",
    "valid_size = 10000\n",
    "idx = range(train.shape[0])\n",
    "np.random.shuffle(idx)\n",
    "\n",
    "valid_data = train[idx[0:valid_size], :]\n",
    "valid_labels = labels[idx[0:valid_size], :]\n",
    "\n",
    "train_data = train[idx[valid_size:], :]\n",
    "train_labels = labels[idx[valid_size:], :]\n",
    "\n",
    "print \"all data :\\t\",train.shape\n",
    "print \"all labels:\\t\",labels.shape\n",
    "print \"train data :\\t\",train_data.shape\n",
    "print \"train labels:\\t\",train_labels.shape\n",
    "print \"valid data :\\t\",valid_data.shape\n",
    "print \"valid labels:\\t\",valid_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.float32(-3.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def eval_gini(y_true, y_prob):\n",
    "    y_true = y_true[np.argsort(y_prob)]\n",
    "    ntrue = 0\n",
    "    gini = 0\n",
    "    delta = 0\n",
    "    n = len(y_true)\n",
    "    for i in range(n-1, -1, -1):\n",
    "        y_i = y_true[i]\n",
    "        ntrue += y_i\n",
    "        gini += y_i * delta\n",
    "        delta += 1 - y_i\n",
    "    gini = 1 - 2 * gini / (ntrue * (n - ntrue))\n",
    "    return gini\n",
    "\n",
    "def gini_loss(y_true, y_prob) :\n",
    "    return np.float32( - eval_gini(np.asarray(y_true[:,0], dtype=np.float32), \n",
    "                                   np.asarray(y_prob[:,0], dtype=np.float32)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################\n",
    "# extract batch_data and batch_label at each epoch #\n",
    "####################################################\n",
    "def extract_batch(i, sym=False, true_perc=0):\n",
    "    if sym :\n",
    "        # uses i, true_perc, train_true, train_false and batch_size\n",
    "        n_t = int(batch_size * true_perc)\n",
    "        n_f = batch_size - n_t\n",
    "       \n",
    "        offset_T = (i * n_t) % (train_true.shape[0] - n_t)\n",
    "        offset_F = (i * n_f) % (train_false.shape[0] - n_f)\n",
    "        \n",
    "        batch_data = train_true[ offset_T : (offset_T+n_t), : ]\n",
    "        batch_data = np.append(batch_data, train_false[offset_F : (offset_F+n_f), :], axis=0)\n",
    "        \n",
    "        batch_labels = np.ones([n_t,1])\n",
    "        batch_labels = np.append(batch_labels, np.zeros([n_f,1]), axis=0)\n",
    "        \n",
    "    else :\n",
    "        # uses i, train_data, train_labels, and batch_size\n",
    "        offset = (i * batch_size) % (train_data.shape[0] - batch_size)\n",
    "        batch_data = train_data[ offset : (offset+batch_size), : ]\n",
    "        batch_labels = train_labels[ offset : (offset+batch_size), : ]\n",
    "    \n",
    "    # shuffle data\n",
    "    shuffle = range(batch_size)\n",
    "    np.random.shuffle(shuffle)\n",
    "    batch_data = batch_data[shuffle]\n",
    "    batch_labels = batch_labels[shuffle]\n",
    "    \n",
    "    return batch_data, batch_labels.reshape(-1,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################\n",
    "# Neural Network's Grapth #\n",
    "###########################\n",
    "batch_size = 64\n",
    "n_features = 123\n",
    "starter_learning_rate = .1\n",
    "keep_prob = 1\n",
    "l1_s = 7\n",
    "l2_s = 7\n",
    "l3_s = 5\n",
    "l4_s = 5\n",
    "l5_s = 5\n",
    "l6_s = 2\n",
    "epsilons = [1, 1e-1, 1e-5]\n",
    "epsilon = 1e-3\n",
    "beta = 0.01\n",
    "\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "    \n",
    "    tf_train_dataset = tf.placeholder(tf.float32, shape=[batch_size, n_features])\n",
    "    tf_train_labels  = tf.placeholder(tf.int32, shape=[batch_size])\n",
    "    tf_valid_dataset = tf.constant(valid_data, dtype=tf.float32)\n",
    "    #tf_test_dataset  = tf.constant(test_data, dtype=tf.float32)\n",
    "    \n",
    "    W1 = tf.Variable(tf.truncated_normal( [n_features, l1_s], stddev=np.sqrt(2.0/n_features) ))\n",
    "    W2 = tf.Variable(tf.truncated_normal( [l1_s, l2_s], stddev=np.sqrt(2./l1_s) ))\n",
    "    W3 = tf.Variable(tf.truncated_normal( [l2_s, l3_s], stddev=np.sqrt(2./l2_s) ))\n",
    "    W4 = tf.Variable(tf.truncated_normal( [l3_s, l4_s], stddev=np.sqrt(2./l3_s) ))\n",
    "    W5 = tf.Variable(tf.truncated_normal( [l4_s, l5_s], stddev=np.sqrt(2./l4_s) ))\n",
    "    W6 = tf.Variable(tf.truncated_normal( [l5_s, l6_s], stddev=np.sqrt(2./l5_s) ))\n",
    "    \n",
    "    b1 = tf.Variable(tf.zeros(l1_s))\n",
    "    b2 = tf.Variable(tf.zeros(l2_s))\n",
    "    b3 = tf.Variable(tf.zeros(l3_s))\n",
    "    b4 = tf.Variable(tf.zeros(l4_s))\n",
    "    b5 = tf.Variable(tf.zeros(l5_s))\n",
    "    b6 = tf.Variable(tf.zeros(l6_s))\n",
    "    #---------------------------------------------------------\n",
    "    def batch_norm_wrapper(X,is_training, decay=0.999) :\n",
    "        scale = tf.Variable(tf.ones(X.shape[-1]))\n",
    "        offset  = tf.Variable(tf.zeros(X.shape[-1]))\n",
    "        pop_mean = tf.Variable(tf.zeros(X.shape[-1]), trainable=False)\n",
    "        pop_variance = tf.Variable(tf.ones(X.shape[-1]), trainable=False)\n",
    "        \n",
    "        if is_training :\n",
    "            batch_mean, batch_variance = tf.nn.moments(X,[0])\n",
    "            train_mean = tf.assign(pop_mean, pop_mean * decay + batch_mean * (1 - decay))\n",
    "            train_variance = tf.assign(pop_variance, pop_variance * decay + batch_variance * (1 - decay))\n",
    "            with tf.control_dependencies([train_mean, train_variance]) :\n",
    "                return tf.nn.batch_normalization(X, batch_mean, batch_variance, offset, scale, epsilon)\n",
    "        else :\n",
    "            return tf.nn.batch_normalization(X, pop_mean, pop_variance, offset, scale, epsilon)  \n",
    "    #--------------------------------------------------------------------------\n",
    "    def forward_propagation(X, is_training=True) :\n",
    "        l1_logits = tf.matmul(X, W1) + b1\n",
    "        l1_BN     = batch_norm_wrapper(l1_logits, is_training)\n",
    "        l1_relu   = tf.nn.relu(l1_BN)\n",
    "        if is_training : l1_relu = tf.nn.dropout(l1_relu, keep_prob)\n",
    "\n",
    "        l2_logits = tf.matmul(l1_relu, W2) + b2\n",
    "        l2_BN     = batch_norm_wrapper(l2_logits, is_training)\n",
    "        l2_relu   = tf.nn.relu(l2_BN)\n",
    "        if is_training : l2_relu = tf.nn.dropout(l2_relu, keep_prob)\n",
    "\n",
    "        l3_logits = tf.matmul(l2_relu, W3) + b3\n",
    "        l3_BN     = batch_norm_wrapper(l3_logits, is_training)\n",
    "        l3_relu   = tf.nn.relu(l3_BN)\n",
    "        if is_training : l3_relu = tf.nn.dropout(l3_relu, keep_prob)\n",
    "            \n",
    "        l4_logits = tf.matmul(l3_relu, W4) + b4\n",
    "        l4_BN     = batch_norm_wrapper(l4_logits, is_training)\n",
    "        l4_relu   = tf.nn.relu(l4_BN)\n",
    "        if is_training : l4_relu = tf.nn.dropout(l4_relu, keep_prob)\n",
    "            \n",
    "        l5_logits = tf.matmul(l4_relu, W5) + b5\n",
    "        l5_BN     = batch_norm_wrapper(l5_logits, is_training)\n",
    "        l5_relu   = tf.nn.relu(l5_BN)\n",
    "        if is_training : l5_relu = tf.nn.dropout(l5_relu, keep_prob)    \n",
    "            \n",
    "\n",
    "        l6_logits = tf.matmul(l5_relu, W6) + b6\n",
    "    \n",
    "        return l6_logits\n",
    "    #----------------------------------------------------------------------\n",
    "    logits = forward_propagation(tf_train_dataset)\n",
    "    #---------------------------------------------------------------------------------------\n",
    "    # LOSS\n",
    "    # Applying Sigmoid function to get loss. \n",
    "    loss = tf.reduce_mean(\\\n",
    "        tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits,labels=tf_train_labels))\n",
    "    #loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=logits, labels=tf_train_labels))  \n",
    "    #----------------------------------------------------------------------------\n",
    "    # L2 regularzation\n",
    "    #regularizers = tf.nn.l2_loss(W1) + tf.nn.l2_loss(W2) + tf.nn.l2_loss(W3) + tf.nn.l2_loss(W4) +\\\n",
    "    #    tf.nn.l2_loss(W5) + tf.nn.l2_loss(W6)\n",
    "    #loss = tf.reduce_mean(loss + beta * regularizers)\n",
    "    #-----------------------------------------------------------------\n",
    "    # Optimizer\n",
    "    # Adam optimizer\n",
    "    optimizer = tf.train.AdamOptimizer().minimize(loss)\n",
    "    #----------------------------------------------------------------------------\n",
    "    # Predictions\n",
    "    train_predictions = tf.nn.sigmoid(logits)\n",
    "    \n",
    "    #validation predictions\n",
    "    logits = forward_propagation(tf_valid_dataset, is_training=False)\n",
    "    valid_predictions = tf.nn.sigmoid(logits)\n",
    "    \n",
    "    #test data predictions\n",
    "    #logits = forward_propagation(tf_test_dataset, is_training=False)\n",
    "    #test_predictions = tf.nn.sigmoid(logits)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#####################################\n",
    "#   Neural Network's Session        #\n",
    "#####################################\n",
    "num_steps = int(5e5+1)\n",
    "losses = []\n",
    "#test_pred = np.zeros(test_data.shape[0])\n",
    "st = 0;\n",
    "\n",
    "with tf.Session(graph=graph) as sess :\n",
    "    \n",
    "    tf.global_variables_initializer().run()\n",
    "    print \"initialized\"\n",
    "    \n",
    "    for i in range(num_steps) :\n",
    "        \n",
    "        batch_data, batch_labels = extract_batch(i, True, 0.5)\n",
    "        #batch_data, batch_labels = train_data, train_labels.reshape(-1,)\n",
    "        feed_dict = { tf_train_dataset : batch_data, tf_train_labels : batch_labels }        \n",
    "        _, l, train_pred = sess.run([optimizer, loss, train_predictions], feed_dict=feed_dict)\n",
    "        \n",
    "\n",
    "        #if i >= 0 :\n",
    "        if i % int(1e4) == 0 :\n",
    "            \n",
    "            losses.append(l)\n",
    "            \n",
    "            train_gini_score = eval_gini(batch_labels, train_pred[:,1])\n",
    "            train_f1_score = f1_score(y_true=batch_labels, y_pred=[ p.argmax() for p in train_pred ])\n",
    "\n",
    "            valid_pred = valid_predictions.eval()\n",
    "            valid_gini_score = eval_gini(valid_labels, valid_pred[:,1])\n",
    "            valid_f1_score = f1_score(y_true=valid_labels, y_pred=[ p.argmax() for p in valid_pred ])\n",
    "            \n",
    "            print \"\\nDone %d steps with loss : \\t%.4f\" % (i, l)\n",
    "            print \"Gini on training data : \\t%.4f\\t and f1 score of : %.4f\" %\\\n",
    "                (train_gini_score, train_f1_score)\n",
    "            print \"Gini on validation data : \\t%.4f\\t and f1 score of : %.4f\" %\\\n",
    "                (valid_gini_score, valid_f1_score)\n",
    "            \n",
    "            #if i > int(1e5) :\n",
    "                #st += 1\n",
    "                #test_pred += test_predictions.eval()[:,1]\n",
    "    #test_pred /= st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,5))\n",
    "plt.plot(range(1,len(losses)),losses[1:])\n",
    "plt.title(\"Model Loss\")\n",
    "plt.xlabel('epoch x 1000')\n",
    "plt.ylabel('loss')\n",
    "plt.legend(['train'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(range(0,1000*len(losses),1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_gini(batch_labels[:,0], train_pred[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pred[train_pred < .9] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pred[np.isnan(train_pred)] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_pred[np.isnan(valid_pred)] = 0\n",
    "            valid_pred[valid_pred >= 0.5] = 1\n",
    "            valid_pred[valid_pred < 0.5] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = np.load('./data/test_ids.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/submission.csv', 'wb') as f :\n",
    "\tf.write('id,target')\n",
    "\tfor i,p in zip(ids,test_pred) :\n",
    "\t\tf.write('\\n%d,%.6f'%(i,p))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.min([ p.argmax() for p in valid_pred ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('./data/test_ids.npy', ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_pred[0].argmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [p[1] for p in test_pred]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = train_pred[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = tf.Graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with g.as_default():\n",
    "    lab = tf.constant(batch_labels)\n",
    "    pr = tf.constant(train_pred)\n",
    "    #pr = tf.nn.sigmoid_cross_entropy_with_logits(logits=logits, labels=tf_train_labels)\n",
    "\n",
    "    a = tf.py_func(eval_gini, (lab[:,0], pr[:,0]), tf.double)\n",
    "    print a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session(graph=g):\n",
    "    a.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
